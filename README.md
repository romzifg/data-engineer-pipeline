# End-to-End ETL Pipeline with Apache Airflow

## ğŸ“Œ Project Overview
This project implements an end-to-end **ETL (Extract, Transform, Load) pipeline** using **Apache Airflow** to process e-commerce cart data.

The pipeline is developed and executed in a **Linux environment using WSL (Windows Subsystem for Linux)**, with a **Python virtual environment (`.venv`)** for dependency isolation during local development and testing.  
The system is fully containerized using Docker for orchestration and database services.

---

## ğŸ§  Problem Statement
Raw transactional data from external APIs is often not analytics-ready.
It may contain missing values, inconsistent data types, and lacks derived metrics.

This project addresses those challenges by building a reliable ETL pipeline that:
- Ingests data from a REST API
- Cleans and transforms raw data
- Loads structured data into a PostgreSQL data warehouse
- Is orchestrated and monitored using Apache Airflow

---

## ğŸ¯ Objective
- Build an Airflow-based ETL pipeline
- Run the project in a Linux environment via WSL
- Use a Python virtual environment for dependency isolation
- Separate infrastructure concerns using Docker
- Automate extract, transform, and load processes
- Produce analytics-ready data

---

## ğŸ— Architecture Diagram

![ETL Flow](diagrams/etl_flow.png)

---

## ğŸ”„ ETL Pipeline Flow

### 1ï¸âƒ£ Extract
- Located in `docker/airflow/etl/extract.py`
- Fetches cart data from a public REST API (`dummyjson.com`)
- Stores raw data as CSV in the data layer

### 2ï¸âƒ£ Transform
- Located in `docker/airflow/etl/transform.py`
- Removes null values
- Ensures correct data types
- Adds derived metric `discount_rate`
- Outputs a clean CSV file

### 3ï¸âƒ£ Load
- Located in `docker/airflow/etl/load.py`
- Loads transformed data into PostgreSQL
- Creates fact table `fact_carts`

---

## ğŸ›  Tech Stack

| Layer | Technology |
|------|-----------|
Orchestration | Apache Airflow |
Data Processing | Python (Pandas) |
Data Source | REST API |
Data Warehouse | PostgreSQL |
Infrastructure | Docker & Docker Compose |
Environment | WSL (Linux) + Python Virtual Environment |
OS | Windows (via WSL) |

---

## ğŸ“‚ Project Structure

project-root/
â”‚
â”œâ”€â”€ .venv/ # Python virtual environment (WSL - local development)
â”‚
â”œâ”€â”€ docker/
â”‚ â”œâ”€â”€ airflow/
â”‚ â”‚ â”œâ”€â”€ dags/
â”‚ â”‚ â”‚ â””â”€â”€ ecommerce_pipeline.py
â”‚ â”‚ â”œâ”€â”€ etl/
â”‚ â”‚ â”‚ â”œâ”€â”€ extract.py
â”‚ â”‚ â”‚ â”œâ”€â”€ transform.py
â”‚ â”‚ â”‚ â””â”€â”€ load.py
â”‚ â”‚ â”œâ”€â”€ data/
â”‚ â”‚ â”‚ â”œâ”€â”€ raw_carts.csv
â”‚ â”‚ â”‚ â””â”€â”€ clean_carts.csv
â”‚ â”‚ â””â”€â”€ docker-compose.yml
â”‚ â”‚
â”‚ â””â”€â”€ db/
â”‚ â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ diagrams/
â”‚ â””â”€â”€ etl_flow.png
â”‚
â””â”€â”€ README.md

## â–¶ï¸ Local Development Environment (WSL)

This project is developed and tested using **WSL (Windows Subsystem for Linux)** to ensure a Linux-compatible environment similar to production systems.

A Python virtual environment is used for dependency isolation when running scripts locally:

```bash
python -m venv .venv
source .venv/bin/activate
```

## How To Run (Docker)
### Start PostgreSQL
```bash
cd docker/db
docker-compose up -d
```

### Start Airflow
```bash
cd docker/airflow
docker-compose up -d
```

### Access Airflow UI
```bash
http://localhost:8080
```